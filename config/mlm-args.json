{
  "gradient_accumulation_steps": 20,
  "learning_rate": 0.0002,
  "weight_decay": 0.01,
  "adam_beta1": 0.9,
  "adam_beta2": 0.999,
  "adam_epsilon": 1e-06,
  "max_steps": 100000,
  "warmup_steps": 5000,
  "logging_steps": 1000,
  "save_steps": 1000,
  "save_total_limit": 10,
  "dataloader_num_workers": 1
}